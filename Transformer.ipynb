{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7nmgscAbX5a",
        "outputId": "d502ceaf-2f2d-44bf-f721-2b62b89e89f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow Version: 2.19.0\n",
            "GPU Available: []\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Loading Train Data from: /content/drive/MyDrive/DEEP LEARNING/dataset/dataset_sentimen_train.csv\n",
            "Loading Test Data from: /content/drive/MyDrive/DEEP LEARNING/dataset/dataset_sentimen_test.csv\n",
            "\n",
            "Contoh Data Train:\n",
            "                                         text    label\n",
            "0    Fiturnya sangat rusak tidak recommended.  negatif\n",
            "1  Kualitasnya cukup jelek tidak recommended.  negatif\n",
            "2        Produk tersebut sangat ramah sekali!  positif\n",
            "3                         Ini adalah plastik.   netral\n",
            "4             Lokasinya ada di nasi dan ayam.   netral\n",
            "\n",
            "Mulai Training...\n",
            "Epoch 1/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 66ms/step - accuracy: 0.3705 - loss: 1.5083\n",
            "Epoch 2/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.8688 - loss: 0.5633\n",
            "Epoch 3/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - accuracy: 1.0000 - loss: 0.0061\n",
            "Epoch 4/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - accuracy: 1.0000 - loss: 8.2649e-04\n",
            "Epoch 5/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 1.0000 - loss: 4.3034e-04\n",
            "Epoch 6/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - accuracy: 1.0000 - loss: 3.7436e-04\n",
            "Epoch 7/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 1.0000 - loss: 2.9255e-04\n",
            "Epoch 8/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 1.0000 - loss: 3.0027e-04\n",
            "Epoch 9/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 1.0000 - loss: 2.4268e-04\n",
            "Epoch 10/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - accuracy: 1.0000 - loss: 2.2932e-04\n",
            "Epoch 11/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - accuracy: 1.0000 - loss: 2.2283e-04\n",
            "Epoch 12/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - accuracy: 1.0000 - loss: 1.9367e-04\n",
            "Epoch 13/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 1.0000 - loss: 1.8313e-04\n",
            "Epoch 14/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 1.0000 - loss: 1.6557e-04\n",
            "Epoch 15/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 1.0000 - loss: 1.5618e-04\n",
            "Epoch 16/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 1.0000 - loss: 1.4984e-04\n",
            "Epoch 17/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - accuracy: 1.0000 - loss: 1.4028e-04\n",
            "Epoch 18/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 1.0000 - loss: 1.3641e-04\n",
            "Epoch 19/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 1.0000 - loss: 1.2697e-04\n",
            "Epoch 20/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - accuracy: 1.0000 - loss: 1.1606e-04\n",
            "\n",
            "Prediksi Data Test...\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step\n",
            "\n",
            "SUKSES!\n",
            "Hasil tersimpan di: /content/drive/MyDrive/DEEP LEARNING/dataset/hasil_prediksi_sentimen_transformer.csv\n",
            "                                                text prediksi_label\n",
            "0             Produk tersebut agak memuaskan banget.        positif\n",
            "1                  Menu terdiri dari 50 ribu rupiah.         netral\n",
            "2         Kualitasnya terlalu rusak tolong perbaiki.        negatif\n",
            "3  Produk tersebut benar-benar berantakan tidak r...        negatif\n",
            "4             Pengirimannya benar-benar ribet parah.        negatif\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# 1. PERSIAPAN & IMPORT LIBRARY\n",
        "# ==========================================\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Pastikan TensorFlow menggunakan GPU jika tersedia\n",
        "print(\"TensorFlow Version:\", tf.__version__)\n",
        "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
        "\n",
        "# ==========================================\n",
        "# 2. LOAD DATASET (Google Drive)\n",
        "# ==========================================\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "BASE_PATH = '/content/drive/MyDrive/DEEP LEARNING/dataset'\n",
        "\n",
        "train_path = os.path.join(BASE_PATH, 'dataset_sentimen_train.csv')\n",
        "test_path  = os.path.join(BASE_PATH, 'dataset_sentimen_test.csv')\n",
        "\n",
        "print(f\"Loading Train Data from: {train_path}\")\n",
        "df_train = pd.read_csv(train_path)\n",
        "\n",
        "print(f\"Loading Test Data from: {test_path}\")\n",
        "df_test = pd.read_csv(test_path)\n",
        "\n",
        "print(\"\\nContoh Data Train:\")\n",
        "print(df_train.head())\n",
        "\n",
        "# ==========================================\n",
        "# 3. PRE-PROCESSING\n",
        "# ==========================================\n",
        "# A. Label Encoding\n",
        "le = LabelEncoder()\n",
        "train_labels = le.fit_transform(df_train['label'])\n",
        "num_classes = len(np.unique(train_labels))\n",
        "\n",
        "# B. Tokenisasi & Padding\n",
        "vocab_size = 5000\n",
        "max_len = 50\n",
        "embedding_dim = 64\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(df_train['text'])\n",
        "\n",
        "train_seq = tokenizer.texts_to_sequences(df_train['text'])\n",
        "test_seq  = tokenizer.texts_to_sequences(df_test['text'])\n",
        "\n",
        "x_train = pad_sequences(train_seq, maxlen=max_len, padding='post', truncating='post')\n",
        "x_test  = pad_sequences(test_seq,  maxlen=max_len, padding='post', truncating='post')\n",
        "\n",
        "# ==========================================\n",
        "# 4. KOMPONEN TRANSFORMER\n",
        "# ==========================================\n",
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "    def __init__(self, vocab_size, d_model, max_len):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_embedding = tf.keras.layers.Embedding(max_len, d_model)\n",
        "\n",
        "    def call(self, x):\n",
        "        positions = tf.range(start=0, limit=tf.shape(x)[1], delta=1)\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        return x + self.pos_embedding(positions)\n",
        "\n",
        "class CausalSelfAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.mha = tf.keras.layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=d_model\n",
        "        )\n",
        "\n",
        "    def call(self, x):\n",
        "        return self.mha(x, x, use_causal_mask=True)\n",
        "\n",
        "def feed_forward(d_model, dff):\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(dff, activation='relu'),\n",
        "        tf.keras.layers.Dense(d_model)\n",
        "    ])\n",
        "\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate):\n",
        "        super().__init__()\n",
        "        self.att = CausalSelfAttention(d_model, num_heads)\n",
        "        self.ffn = feed_forward(d_model, dff)\n",
        "        self.norm1 = tf.keras.layers.LayerNormalization()\n",
        "        self.norm2 = tf.keras.layers.LayerNormalization()\n",
        "        self.drop1 = tf.keras.layers.Dropout(rate)\n",
        "        self.drop2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training):\n",
        "        attn = self.att(x)\n",
        "        x = self.norm1(x + self.drop1(attn, training=training))\n",
        "        ffn = self.ffn(x)\n",
        "        return self.norm2(x + self.drop2(ffn, training=training))\n",
        "\n",
        "class SentimentTransformer(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, dff, num_classes, max_len, rate):\n",
        "        super().__init__()\n",
        "        self.embed = PositionalEmbedding(vocab_size, d_model, max_len)\n",
        "        # Renamed 'layers' to 'transformer_layers' to avoid conflict with Keras Model's reserved 'layers' attribute\n",
        "        self.transformer_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "        self.pool = tf.keras.layers.GlobalAveragePooling1D()\n",
        "        self.out = tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "\n",
        "    def call(self, x, training):\n",
        "        x = self.embed(x)\n",
        "        for layer in self.transformer_layers:\n",
        "            x = layer(x, training=training) # Pass training as a keyword argument\n",
        "        x = self.pool(x)\n",
        "        return self.out(x)\n",
        "\n",
        "# ==========================================\n",
        "# 5. TRAINING MODEL\n",
        "# ==========================================\n",
        "model = SentimentTransformer(\n",
        "    vocab_size=vocab_size,\n",
        "    d_model=embedding_dim,\n",
        "    num_layers=2,\n",
        "    num_heads=4,\n",
        "    dff=128,\n",
        "    num_classes=num_classes,\n",
        "    max_len=max_len,\n",
        "    rate=0.1\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"\\nMulai Training...\")\n",
        "model.fit(x_train, train_labels, epochs=20, batch_size=16)\n",
        "\n",
        "# ==========================================\n",
        "# 6. PREDIKSI & SIMPAN HASIL\n",
        "# ==========================================\n",
        "print(\"\\nPrediksi Data Test...\")\n",
        "pred = model.predict(x_test)\n",
        "pred_idx = np.argmax(pred, axis=1)\n",
        "df_test['prediksi_label'] = le.inverse_transform(pred_idx)\n",
        "\n",
        "output_path = os.path.join(BASE_PATH, 'hasil_prediksi_sentimen_transformer.csv')\n",
        "df_test.to_csv(output_path, index=False)\n",
        "\n",
        "print(\"\\nSUKSES!\")\n",
        "print(f\"Hasil tersimpan di: {output_path}\")\n",
        "print(df_test.head())"
      ]
    }
  ]
}